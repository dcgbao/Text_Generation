{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcxqGls7iJ3RXEsOw5LMJM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Download Dataset**"
      ],
      "metadata": {
        "id": "pSU2Uu_kr-aZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WpELhCquLqlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbaa3af-08d2-429f-a2ac-caabb57ca8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-13 01:07:36--  https://storage.googleapis.com/protonx-cloud-storage/data.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.207, 108.177.98.207, 74.125.197.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "\rdata.txt              0%[                    ]       0  --.-KB/s               \rdata.txt            100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-13 01:07:37 (91.9 MB/s) - ‘data.txt’ saved [93578/93578]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/protonx-cloud-storage/data.txt\n",
        "data = open('data.txt').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Libraries**"
      ],
      "metadata": {
        "id": "ItHSj-AZyzIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras.utils as ku\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "eAjg9Htdy2lO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Data Preprocessing**"
      ],
      "metadata": {
        "id": "ShHDwu3CE_xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = data.lower().split(\"\\n\")"
      ],
      "metadata": {
        "id": "t100zxkUFFWj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE9nWFFtF1pi",
        "outputId": "798cd320-59a3-43a0-d221-9fd7ab23483c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from fairest creatures we desire increase,',\n",
              " \"that thereby beauty's rose might never die,\",\n",
              " 'but as the riper should by time decease,',\n",
              " 'his tender heir might bear his memory:',\n",
              " 'but thou, contracted to thine own bright eyes,',\n",
              " \"feed'st thy light'st flame with self-substantial fuel,\",\n",
              " 'making a famine where abundance lies,',\n",
              " 'thyself thy foe, to thy sweet self too cruel.',\n",
              " \"thou that art now the world's fresh ornament\",\n",
              " 'and only herald to the gaudy spring,']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1. Build Vocabulary**"
      ],
      "metadata": {
        "id": "7tAv3pj3GO-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)"
      ],
      "metadata": {
        "id": "FvDBOfmwGY1f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXFruZyIn16",
        "outputId": "692d39bb-57e0-4473-9081-26beb44b3863"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 1,\n",
              " 'the': 2,\n",
              " 'to': 3,\n",
              " 'of': 4,\n",
              " 'my': 5,\n",
              " 'i': 6,\n",
              " 'in': 7,\n",
              " 'that': 8,\n",
              " 'thy': 9,\n",
              " 'thou': 10,\n",
              " 'with': 11,\n",
              " 'for': 12,\n",
              " 'is': 13,\n",
              " 'love': 14,\n",
              " 'not': 15,\n",
              " 'but': 16,\n",
              " 'a': 17,\n",
              " 'me': 18,\n",
              " 'thee': 19,\n",
              " 'so': 20,\n",
              " 'be': 21,\n",
              " 'as': 22,\n",
              " 'all': 23,\n",
              " 'you': 24,\n",
              " 'his': 25,\n",
              " 'which': 26,\n",
              " 'when': 27,\n",
              " 'it': 28,\n",
              " 'this': 29,\n",
              " 'by': 30,\n",
              " 'your': 31,\n",
              " 'doth': 32,\n",
              " 'do': 33,\n",
              " 'from': 34,\n",
              " 'on': 35,\n",
              " 'or': 36,\n",
              " 'no': 37,\n",
              " 'then': 38,\n",
              " 'have': 39,\n",
              " 'what': 40,\n",
              " 'are': 41,\n",
              " 'if': 42,\n",
              " 'more': 43,\n",
              " 'mine': 44,\n",
              " 'their': 45,\n",
              " 'shall': 46,\n",
              " 'sweet': 47,\n",
              " 'time': 48,\n",
              " 'will': 49,\n",
              " 'they': 50,\n",
              " 'beauty': 51,\n",
              " 'nor': 52,\n",
              " 'eyes': 53,\n",
              " 'art': 54,\n",
              " 'her': 55,\n",
              " 'heart': 56,\n",
              " 'yet': 57,\n",
              " 'o': 58,\n",
              " 'than': 59,\n",
              " 'can': 60,\n",
              " 'should': 61,\n",
              " 'thine': 62,\n",
              " 'now': 63,\n",
              " 'where': 64,\n",
              " 'make': 65,\n",
              " 'one': 66,\n",
              " 'hath': 67,\n",
              " 'he': 68,\n",
              " 'fair': 69,\n",
              " 'still': 70,\n",
              " 'how': 71,\n",
              " 'eye': 72,\n",
              " 'him': 73,\n",
              " 'like': 74,\n",
              " 'true': 75,\n",
              " 'see': 76,\n",
              " 'am': 77,\n",
              " 'she': 78,\n",
              " 'those': 79,\n",
              " 'though': 80,\n",
              " 'being': 81,\n",
              " 'some': 82,\n",
              " 'every': 83,\n",
              " 'such': 84,\n",
              " 'own': 85,\n",
              " 'were': 86,\n",
              " 'dost': 87,\n",
              " 'who': 88,\n",
              " 'live': 89,\n",
              " 'upon': 90,\n",
              " 'was': 91,\n",
              " 'may': 92,\n",
              " 'myself': 93,\n",
              " 'world': 94,\n",
              " 'say': 95,\n",
              " 'praise': 96,\n",
              " 'day': 97,\n",
              " 'new': 98,\n",
              " 'give': 99,\n",
              " 'most': 100,\n",
              " \"love's\": 101,\n",
              " 'might': 102,\n",
              " 'did': 103,\n",
              " 'let': 104,\n",
              " 'well': 105,\n",
              " 'at': 106,\n",
              " 'why': 107,\n",
              " 'since': 108,\n",
              " 'even': 109,\n",
              " 'show': 110,\n",
              " 'life': 111,\n",
              " 'old': 112,\n",
              " 'look': 113,\n",
              " 'best': 114,\n",
              " 'night': 115,\n",
              " 'dear': 116,\n",
              " 'truth': 117,\n",
              " 'thyself': 118,\n",
              " 'must': 119,\n",
              " 'would': 120,\n",
              " 'thus': 121,\n",
              " 'self': 122,\n",
              " 'ill': 123,\n",
              " 'part': 124,\n",
              " 'these': 125,\n",
              " 'worth': 126,\n",
              " 'made': 127,\n",
              " 'face': 128,\n",
              " 'whose': 129,\n",
              " 'nothing': 130,\n",
              " 'alone': 131,\n",
              " 'false': 132,\n",
              " 'better': 133,\n",
              " \"beauty's\": 134,\n",
              " 'too': 135,\n",
              " 'much': 136,\n",
              " 'there': 137,\n",
              " 'hand': 138,\n",
              " 'thought': 139,\n",
              " 'away': 140,\n",
              " 'against': 141,\n",
              " 'them': 142,\n",
              " 'thoughts': 143,\n",
              " 'our': 144,\n",
              " 'days': 145,\n",
              " 'an': 146,\n",
              " \"'\": 147,\n",
              " 'up': 148,\n",
              " 'sight': 149,\n",
              " 'out': 150,\n",
              " 'hast': 151,\n",
              " 'know': 152,\n",
              " 'therefore': 153,\n",
              " 'both': 154,\n",
              " 'name': 155,\n",
              " 'never': 156,\n",
              " 'death': 157,\n",
              " 'mind': 158,\n",
              " \"time's\": 159,\n",
              " 'other': 160,\n",
              " 'find': 161,\n",
              " 'had': 162,\n",
              " 'muse': 163,\n",
              " 'far': 164,\n",
              " 'dead': 165,\n",
              " 'we': 166,\n",
              " 'tell': 167,\n",
              " 'age': 168,\n",
              " 'each': 169,\n",
              " 'youth': 170,\n",
              " 'good': 171,\n",
              " 'men': 172,\n",
              " 'before': 173,\n",
              " 'verse': 174,\n",
              " 'come': 175,\n",
              " 'tongue': 176,\n",
              " 'poor': 177,\n",
              " 'think': 178,\n",
              " 'proud': 179,\n",
              " 'gentle': 180,\n",
              " 'wilt': 181,\n",
              " 'state': 182,\n",
              " 'till': 183,\n",
              " 'things': 184,\n",
              " 'friend': 185,\n",
              " 'use': 186,\n",
              " 'many': 187,\n",
              " 'prove': 188,\n",
              " 'none': 189,\n",
              " 'hate': 190,\n",
              " 'hold': 191,\n",
              " 'heaven': 192,\n",
              " 'first': 193,\n",
              " 'whilst': 194,\n",
              " 'black': 195,\n",
              " 'lie': 196,\n",
              " 'full': 197,\n",
              " 'take': 198,\n",
              " 'die': 199,\n",
              " 'bear': 200,\n",
              " 'making': 201,\n",
              " 'lies': 202,\n",
              " 'hours': 203,\n",
              " 'looks': 204,\n",
              " 'change': 205,\n",
              " 'kind': 206,\n",
              " 'mayst': 207,\n",
              " 'whom': 208,\n",
              " 'thing': 209,\n",
              " 'long': 210,\n",
              " 'ever': 211,\n",
              " 'woe': 212,\n",
              " 'desire': 213,\n",
              " 'bright': 214,\n",
              " 'within': 215,\n",
              " 'form': 216,\n",
              " \"summer's\": 217,\n",
              " 'pleasure': 218,\n",
              " 'happy': 219,\n",
              " 'shalt': 220,\n",
              " 'end': 221,\n",
              " 'others': 222,\n",
              " 'any': 223,\n",
              " \"o'er\": 224,\n",
              " 'yourself': 225,\n",
              " 'after': 226,\n",
              " 'rich': 227,\n",
              " 'knows': 228,\n",
              " 'earth': 229,\n",
              " 'sun': 230,\n",
              " 'seem': 231,\n",
              " \"'tis\": 232,\n",
              " 'grace': 233,\n",
              " 'pride': 234,\n",
              " 'seen': 235,\n",
              " 'shame': 236,\n",
              " 'glass': 237,\n",
              " 'another': 238,\n",
              " 'great': 239,\n",
              " 'nature': 240,\n",
              " 'leave': 241,\n",
              " 'place': 242,\n",
              " 'could': 243,\n",
              " 'call': 244,\n",
              " 'again': 245,\n",
              " 'pen': 246,\n",
              " 'write': 247,\n",
              " 'once': 248,\n",
              " 'words': 249,\n",
              " 'loving': 250,\n",
              " 'deeds': 251,\n",
              " 'found': 252,\n",
              " 'fire': 253,\n",
              " 'right': 254,\n",
              " 'spirit': 255,\n",
              " 'soul': 256,\n",
              " 'pity': 257,\n",
              " 'treasure': 258,\n",
              " 'back': 259,\n",
              " 'beauteous': 260,\n",
              " 'gone': 261,\n",
              " 'lives': 262,\n",
              " 'times': 263,\n",
              " 'strong': 264,\n",
              " 'keep': 265,\n",
              " 'without': 266,\n",
              " 'decay': 267,\n",
              " 'store': 268,\n",
              " 'past': 269,\n",
              " 'stay': 270,\n",
              " 'lose': 271,\n",
              " 'makes': 272,\n",
              " 'loss': 273,\n",
              " 'two': 274,\n",
              " 'although': 275,\n",
              " 'cannot': 276,\n",
              " 'power': 277,\n",
              " 'memory': 278,\n",
              " 'cruel': 279,\n",
              " 'brow': 280,\n",
              " 'deep': 281,\n",
              " 'child': 282,\n",
              " 'blood': 283,\n",
              " 'lovely': 284,\n",
              " 'gives': 285,\n",
              " 'summer': 286,\n",
              " 'leaves': 287,\n",
              " 'flowers': 288,\n",
              " 'joy': 289,\n",
              " 'fear': 290,\n",
              " 'grow': 291,\n",
              " 'save': 292,\n",
              " 'blessed': 293,\n",
              " 'stand': 294,\n",
              " 'lines': 295,\n",
              " 'skill': 296,\n",
              " 'born': 297,\n",
              " 'glory': 298,\n",
              " 'view': 299,\n",
              " 'disgrace': 300,\n",
              " 'speak': 301,\n",
              " 'faults': 302,\n",
              " 'bring': 303,\n",
              " 'loves': 304,\n",
              " 'delight': 305,\n",
              " 'lest': 306,\n",
              " 'sake': 307,\n",
              " 'thence': 308,\n",
              " 'hell': 309,\n",
              " 'sin': 310,\n",
              " 'tender': 311,\n",
              " 'fresh': 312,\n",
              " 'waste': 313,\n",
              " 'excuse': 314,\n",
              " 'cold': 315,\n",
              " 'through': 316,\n",
              " 'canst': 317,\n",
              " 'very': 318,\n",
              " 'same': 319,\n",
              " 'ten': 320,\n",
              " 'living': 321,\n",
              " 'parts': 322,\n",
              " 'sing': 323,\n",
              " 'ah': 324,\n",
              " 'behold': 325,\n",
              " 'white': 326,\n",
              " 'honour': 327,\n",
              " 'outward': 328,\n",
              " 'less': 329,\n",
              " 'wide': 330,\n",
              " 'worst': 331,\n",
              " 'itself': 332,\n",
              " 'breast': 333,\n",
              " 'put': 334,\n",
              " 'shadow': 335,\n",
              " 'sad': 336,\n",
              " \"i'll\": 337,\n",
              " 'forth': 338,\n",
              " 'roses': 339,\n",
              " 'steal': 340,\n",
              " 'whether': 341,\n",
              " 'straight': 342,\n",
              " 'set': 343,\n",
              " 'breath': 344,\n",
              " 'virtue': 345,\n",
              " 'seeing': 346,\n",
              " 'bad': 347,\n",
              " 'three': 348,\n",
              " 'foul': 349,\n",
              " 'swear': 350,\n",
              " 'rose': 351,\n",
              " 'only': 352,\n",
              " 'else': 353,\n",
              " 'due': 354,\n",
              " 'despite': 355,\n",
              " \"nature's\": 356,\n",
              " 'having': 357,\n",
              " 'dwell': 358,\n",
              " 'ere': 359,\n",
              " 'shouldst': 360,\n",
              " 'light': 361,\n",
              " 'head': 362,\n",
              " 'under': 363,\n",
              " 'way': 364,\n",
              " 'unless': 365,\n",
              " 'music': 366,\n",
              " 'hear': 367,\n",
              " 'sweets': 368,\n",
              " 'war': 369,\n",
              " 'lovest': 370,\n",
              " \"'gainst\": 371,\n",
              " 'green': 372,\n",
              " 'go': 373,\n",
              " 'themselves': 374,\n",
              " 'hence': 375,\n",
              " 'longer': 376,\n",
              " 'eternal': 377,\n",
              " 'judgment': 378,\n",
              " 'fortune': 379,\n",
              " 'read': 380,\n",
              " 'shows': 381,\n",
              " 'rhyme': 382,\n",
              " 'compare': 383,\n",
              " 'sometime': 384,\n",
              " 'swift': 385,\n",
              " 'wrong': 386,\n",
              " \"heaven's\": 387,\n",
              " 'dumb': 388,\n",
              " 'writ': 389,\n",
              " 'wit': 390,\n",
              " 'book': 391,\n",
              " 'rest': 392,\n",
              " 'hope': 393,\n",
              " 'blind': 394,\n",
              " 'return': 395,\n",
              " 'wealth': 396,\n",
              " 'precious': 397,\n",
              " 'while': 398,\n",
              " 'enough': 399,\n",
              " 'argument': 400,\n",
              " 'heavy': 401,\n",
              " 'dull': 402,\n",
              " \"heart's\": 403,\n",
              " 'care': 404,\n",
              " 'strange': 405,\n",
              " 'slave': 406,\n",
              " 'been': 407,\n",
              " 'painting': 408,\n",
              " 'taught': 409,\n",
              " 'fears': 410,\n",
              " 'spent': 411,\n",
              " 'sick': 412,\n",
              " 'reason': 413,\n",
              " 'lips': 414,\n",
              " 'mad': 415,\n",
              " \"'will\": 416,\n",
              " 'fairest': 417,\n",
              " \"world's\": 418,\n",
              " 'ornament': 419,\n",
              " 'spring': 420,\n",
              " 'tomb': 421,\n",
              " 'calls': 422,\n",
              " 'wrinkles': 423,\n",
              " 'golden': 424,\n",
              " 'spend': 425,\n",
              " 'lend': 426,\n",
              " 'play': 427,\n",
              " 'winter': 428,\n",
              " 'quite': 429,\n",
              " 'left': 430,\n",
              " 'gracious': 431,\n",
              " 'mortal': 432,\n",
              " 'chide': 433,\n",
              " 'shape': 434,\n",
              " 'beloved': 435,\n",
              " 'least': 436,\n",
              " 'fast': 437,\n",
              " 'year': 438,\n",
              " 'gave': 439,\n",
              " 'gift': 440,\n",
              " 'barren': 441,\n",
              " 'yours': 442,\n",
              " 'rage': 443,\n",
              " 'stars': 444,\n",
              " 'oft': 445,\n",
              " 'doom': 446,\n",
              " 'date': 447,\n",
              " 'holds': 448,\n",
              " 'wish': 449,\n",
              " 'painted': 450,\n",
              " 'keeps': 451,\n",
              " 'graces': 452,\n",
              " 'antique': 453,\n",
              " 'short': 454,\n",
              " 'lived': 455,\n",
              " 'man': 456,\n",
              " 'hue': 457,\n",
              " \"men's\": 458,\n",
              " 'fell': 459,\n",
              " 'purpose': 460,\n",
              " 'truly': 461,\n",
              " 'trust': 462,\n",
              " 'strength': 463,\n",
              " 'speaking': 464,\n",
              " 'wherein': 465,\n",
              " 'done': 466,\n",
              " 'boast': 467,\n",
              " 'buried': 468,\n",
              " 'merit': 469,\n",
              " 'worthy': 470,\n",
              " 'scope': 471,\n",
              " 'moan': 472,\n",
              " 'hearts': 473,\n",
              " \"stol'n\": 474,\n",
              " 'loved': 475,\n",
              " 'birth': 476,\n",
              " 'hide': 477,\n",
              " 'base': 478,\n",
              " 'grief': 479,\n",
              " 'sorrow': 480,\n",
              " 'tears': 481,\n",
              " 'canker': 482,\n",
              " 'sweetest': 483,\n",
              " 'needs': 484,\n",
              " 'spite': 485,\n",
              " 'subject': 486,\n",
              " 'into': 487,\n",
              " 'invention': 488,\n",
              " 'pain': 489,\n",
              " 'absence': 490,\n",
              " 'kill': 491,\n",
              " 'large': 492,\n",
              " 'water': 493,\n",
              " 'slow': 494,\n",
              " 'present': 495,\n",
              " 'motion': 496,\n",
              " 'down': 497,\n",
              " 'says': 498,\n",
              " 'took': 499,\n",
              " 'side': 500,\n",
              " 'need': 501,\n",
              " 'cheek': 502,\n",
              " 'worse': 503,\n",
              " 'near': 504,\n",
              " 'flower': 505,\n",
              " 'faith': 506,\n",
              " 'tied': 507,\n",
              " 'rank': 508,\n",
              " 'brain': 509,\n",
              " 'saw': 510,\n",
              " 'grew': 511,\n",
              " 'brand': 512,\n",
              " 'cure': 513,\n",
              " \"mistress'\": 514,\n",
              " \"'will'\": 515,\n",
              " 'angel': 516,\n",
              " 'increase': 517,\n",
              " 'abundance': 518,\n",
              " 'sum': 519,\n",
              " 'count': 520,\n",
              " 'prime': 521,\n",
              " 'single': 522,\n",
              " 'image': 523,\n",
              " 'lends': 524,\n",
              " 'free': 525,\n",
              " 'given': 526,\n",
              " 'unused': 527,\n",
              " 'used': 528,\n",
              " 'frame': 529,\n",
              " 'substance': 530,\n",
              " \"death's\": 531,\n",
              " 'worms': 532,\n",
              " 'weary': 533,\n",
              " 'mark': 534,\n",
              " 'song': 535,\n",
              " 'behind': 536,\n",
              " 'grant': 537,\n",
              " 'rude': 538,\n",
              " 'heat': 539,\n",
              " 'beauties': 540,\n",
              " 'scythe': 541,\n",
              " 'here': 542,\n",
              " 'lease': 543,\n",
              " 'pluck': 544,\n",
              " 'methinks': 545,\n",
              " 'evil': 546,\n",
              " 'minutes': 547,\n",
              " 'grows': 548,\n",
              " 'wherefore': 549,\n",
              " 'inward': 550,\n",
              " 'believe': 551,\n",
              " 'high': 552,\n",
              " 'numbers': 553,\n",
              " 'touches': 554,\n",
              " \"ne'er\": 555,\n",
              " 'buds': 556,\n",
              " 'shade': 557,\n",
              " 'crime': 558,\n",
              " 'young': 559,\n",
              " 'mistress': 560,\n",
              " 'rehearse': 561,\n",
              " 'rare': 562,\n",
              " 'air': 563,\n",
              " 'babe': 564,\n",
              " 'body': 565,\n",
              " 'want': 566,\n",
              " 'public': 567,\n",
              " 'forgot': 568,\n",
              " 'removed': 569,\n",
              " 'bare': 570,\n",
              " 'respect': 571,\n",
              " 'dare': 572,\n",
              " 'toil': 573,\n",
              " 'tired': 574,\n",
              " 'jewel': 575,\n",
              " 'please': 576,\n",
              " 'clouds': 577,\n",
              " 'contented': 578,\n",
              " 'break': 579,\n",
              " 'lack': 580,\n",
              " 'account': 581,\n",
              " 'holy': 582,\n",
              " 'appear': 583,\n",
              " 'grown': 584,\n",
              " 'sovereign': 585,\n",
              " 'shine': 586,\n",
              " 'hour': 587,\n",
              " 'wound': 588,\n",
              " 'sense': 589,\n",
              " 'thief': 590,\n",
              " 'help': 591,\n",
              " 'report': 592,\n",
              " 'comfort': 593,\n",
              " 'leisure': 594,\n",
              " 'blame': 595,\n",
              " 'taste': 596,\n",
              " 'absent': 597,\n",
              " 'said': 598,\n",
              " 'gain': 599,\n",
              " 'sleep': 600,\n",
              " 'sea': 601,\n",
              " 'assured': 602,\n",
              " 'told': 603,\n",
              " 'seek': 604,\n",
              " 'groan': 605,\n",
              " 'morrow': 606,\n",
              " 'former': 607,\n",
              " 'second': 608,\n",
              " 'knife': 609,\n",
              " 'brass': 610,\n",
              " 'win': 611,\n",
              " 'forsworn': 612,\n",
              " 'tongues': 613,\n",
              " 'add': 614,\n",
              " 'smell': 615,\n",
              " 'weeds': 616,\n",
              " 'common': 617,\n",
              " 'lost': 618,\n",
              " \"others'\": 619,\n",
              " 'knowing': 620,\n",
              " 'cheeks': 621,\n",
              " 'story': 622,\n",
              " 'praises': 623,\n",
              " 'above': 624,\n",
              " 'wretched': 625,\n",
              " 'turn': 626,\n",
              " 'errors': 627,\n",
              " 'lays': 628,\n",
              " 'red': 629,\n",
              " 'just': 630,\n",
              " 'catch': 631,\n",
              " 'laid': 632,\n",
              " 'bath': 633,\n",
              " 'decease': 634,\n",
              " 'heir': 635,\n",
              " 'flame': 636,\n",
              " 'eat': 637,\n",
              " 'grave': 638,\n",
              " 'weed': 639,\n",
              " 'answer': 640,\n",
              " 'repair': 641,\n",
              " 'posterity': 642,\n",
              " \"mother's\": 643,\n",
              " 'april': 644,\n",
              " 'windows': 645,\n",
              " \"remember'd\": 646,\n",
              " 'abuse': 647,\n",
              " 'audit': 648,\n",
              " 'work': 649,\n",
              " 'confounds': 650,\n",
              " 'effect': 651,\n",
              " \"distill'd\": 652,\n",
              " \"winter's\": 653,\n",
              " 'pay': 654,\n",
              " 'happier': 655,\n",
              " 'conquest': 656,\n",
              " 'lo': 657,\n",
              " 'heavenly': 658,\n",
              " 'son': 659,\n",
              " 'receivest': 660,\n",
              " 'ear': 661,\n",
              " 'husband': 662,\n",
              " 'seeming': 663,\n",
              " 'weep': 664,\n",
              " 'kept': 665,\n",
              " 'murderous': 666,\n",
              " 'fairer': 667,\n",
              " 'brave': 668,\n",
              " 'among': 669,\n",
              " 'takes': 670,\n",
              " 'fall': 671,\n",
              " 'father': 672,\n",
              " 'brief': 673,\n",
              " 'rain': 674,\n",
              " 'knowledge': 675,\n",
              " 'constant': 676,\n",
              " 'wouldst': 677,\n",
              " 'nought': 678,\n",
              " 'whereon': 679,\n",
              " 'height': 680,\n",
              " 'wear': 681,\n",
              " 'conceit': 682,\n",
              " 'bloody': 683,\n",
              " 'tyrant': 684,\n",
              " 'means': 685,\n",
              " 'maiden': 686,\n",
              " 'virtuous': 687,\n",
              " 'drawn': 688,\n",
              " \"fill'd\": 689,\n",
              " 'half': 690,\n",
              " 'number': 691,\n",
              " 'shake': 692,\n",
              " 'hot': 693,\n",
              " 'complexion': 694,\n",
              " 'course': 695,\n",
              " 'fade': 696,\n",
              " 'blunt': 697,\n",
              " 'burn': 698,\n",
              " 'forbid': 699,\n",
              " 'draw': 700,\n",
              " \"woman's\": 701,\n",
              " \"women's\": 702,\n",
              " 'woman': 703,\n",
              " 'wert': 704,\n",
              " 'moon': 705,\n",
              " 'cover': 706,\n",
              " 'bearing': 707,\n",
              " 'gavest': 708,\n",
              " 'burden': 709,\n",
              " \"express'd\": 710,\n",
              " 'turns': 711,\n",
              " 'therein': 712,\n",
              " 'cunning': 713,\n",
              " 'favour': 714,\n",
              " 'triumph': 715,\n",
              " 'frown': 716,\n",
              " 'fight': 717,\n",
              " 'thousand': 718,\n",
              " 'razed': 719,\n",
              " 'duty': 720,\n",
              " 'witness': 721,\n",
              " 'wanting': 722,\n",
              " 'star': 723,\n",
              " 'haste': 724,\n",
              " 'bed': 725,\n",
              " \"body's\": 726,\n",
              " 'looking': 727,\n",
              " 'farther': 728,\n",
              " 'blot': 729,\n",
              " 'flatter': 730,\n",
              " 'daily': 731,\n",
              " 'friends': 732,\n",
              " 'almost': 733,\n",
              " 'haply': 734,\n",
              " 'expense': 735,\n",
              " 'survey': 736,\n",
              " 'growing': 737,\n",
              " 'brought': 738,\n",
              " 'style': 739,\n",
              " 'basest': 740,\n",
              " 'ride': 741,\n",
              " 'west': 742,\n",
              " 'alack': 743,\n",
              " 'stain': 744,\n",
              " 'didst': 745,\n",
              " 'bears': 746,\n",
              " 'cross': 747,\n",
              " 'thorns': 748,\n",
              " 'amiss': 749,\n",
              " 'sins': 750,\n",
              " 'fault': 751,\n",
              " 'lawful': 752,\n",
              " 'plea': 753,\n",
              " 'twain': 754,\n",
              " 'remain': 755,\n",
              " 'evermore': 756,\n",
              " \"fortune's\": 757,\n",
              " 'dearest': 758,\n",
              " 'despised': 759,\n",
              " 'vulgar': 760,\n",
              " 'outlive': 761,\n",
              " 'manners': 762,\n",
              " 'greater': 763,\n",
              " 'injury': 764,\n",
              " 'foes': 765,\n",
              " 'because': 766,\n",
              " 'approve': 767,\n",
              " 'losing': 768,\n",
              " 'flattery': 769,\n",
              " 'dark': 770,\n",
              " 'shadows': 771,\n",
              " 'clear': 772,\n",
              " 'flesh': 773,\n",
              " 'matter': 774,\n",
              " 'foot': 775,\n",
              " 'soon': 776,\n",
              " \"eye's\": 777,\n",
              " 'picture': 778,\n",
              " 'sure': 779,\n",
              " 'chest': 780,\n",
              " 'whence': 781,\n",
              " 'prize': 782,\n",
              " 'pass': 783,\n",
              " 'greet': 784,\n",
              " 'reasons': 785,\n",
              " 'desert': 786,\n",
              " 'cause': 787,\n",
              " 'teach': 788,\n",
              " 'speed': 789,\n",
              " 'pace': 790,\n",
              " 'blest': 791,\n",
              " 'tend': 792,\n",
              " 'odour': 793,\n",
              " 'hang': 794,\n",
              " 'stone': 795,\n",
              " 'quick': 796,\n",
              " 'record': 797,\n",
              " 'edge': 798,\n",
              " 'appetite': 799,\n",
              " 'feeding': 800,\n",
              " 'fill': 801,\n",
              " 'ocean': 802,\n",
              " 'shore': 803,\n",
              " 'thrice': 804,\n",
              " 'fool': 805,\n",
              " 'god': 806,\n",
              " 'control': 807,\n",
              " 'doing': 808,\n",
              " 'five': 809,\n",
              " 'character': 810,\n",
              " 'wonder': 811,\n",
              " 'main': 812,\n",
              " \"crown'd\": 813,\n",
              " 'stands': 814,\n",
              " 'mock': 815,\n",
              " 'home': 816,\n",
              " 'elsewhere': 817,\n",
              " 'cost': 818,\n",
              " 'advantage': 819,\n",
              " 'action': 820,\n",
              " 'steel': 821,\n",
              " 'simple': 822,\n",
              " 'infection': 823,\n",
              " 'bastard': 824,\n",
              " \"another's\": 825,\n",
              " 'mend': 826,\n",
              " 'suspect': 827,\n",
              " 'flies': 828,\n",
              " 'mourn': 829,\n",
              " 'vile': 830,\n",
              " 'line': 831,\n",
              " 'thinking': 832,\n",
              " 'birds': 833,\n",
              " 'ground': 834,\n",
              " 'pine': 835,\n",
              " 'eternity': 836,\n",
              " 'acquaintance': 837,\n",
              " 'got': 838,\n",
              " 'double': 839,\n",
              " 'works': 840,\n",
              " 'fame': 841,\n",
              " 'sail': 842,\n",
              " 'anew': 843,\n",
              " 'silence': 844,\n",
              " \"know'st\": 845,\n",
              " 'bonds': 846,\n",
              " 'vow': 847,\n",
              " 'general': 848,\n",
              " \"what's\": 849,\n",
              " \"seem'd\": 850,\n",
              " 'despair': 851,\n",
              " 'growth': 852,\n",
              " 'colour': 853,\n",
              " 'over': 854,\n",
              " 'sinful': 855,\n",
              " 'constancy': 856,\n",
              " 'confined': 857,\n",
              " \"'fair\": 858,\n",
              " 'boy': 859,\n",
              " 'proved': 860,\n",
              " 'proof': 861,\n",
              " 'receives': 862,\n",
              " 'plague': 863,\n",
              " 'minds': 864,\n",
              " 'taken': 865,\n",
              " 'whereto': 866,\n",
              " 'unkind': 867,\n",
              " 'fingers': 868,\n",
              " 'kiss': 869,\n",
              " 'perjured': 870,\n",
              " 'came': 871,\n",
              " \"'i\": 872,\n",
              " \"hate'\": 873,\n",
              " 'sworn': 874,\n",
              " 'conscience': 875,\n",
              " 'oaths': 876,\n",
              " 'creatures': 877,\n",
              " 'thereby': 878,\n",
              " 'riper': 879,\n",
              " 'contracted': 880,\n",
              " 'bud': 881,\n",
              " 'content': 882,\n",
              " 'churl': 883,\n",
              " 'makest': 884,\n",
              " 'winters': 885,\n",
              " \"tatter'd\": 886,\n",
              " 'small': 887,\n",
              " 'held': 888,\n",
              " 'lusty': 889,\n",
              " \"'this\": 890,\n",
              " 'mother': 891,\n",
              " 'womb': 892,\n",
              " 'husbandry': 893,\n",
              " 'fond': 894,\n",
              " 'stop': 895,\n",
              " 'niggard': 896,\n",
              " 'bounteous': 897,\n",
              " 'usurer': 898,\n",
              " 'deceive': 899,\n",
              " 'gaze': 900,\n",
              " 'leads': 901,\n",
              " 'hideous': 902,\n",
              " 'sap': 903,\n",
              " \"check'd\": 904,\n",
              " 'bareness': 905,\n",
              " 'pent': 906,\n",
              " 'walls': 907,\n",
              " 'remembrance': 908,\n",
              " 'meet': 909,\n",
              " 'willing': 910,\n",
              " 'breed': 911,\n",
              " 'depart': 912,\n",
              " 'serving': 913,\n",
              " 'sacred': 914,\n",
              " 'majesty': 915,\n",
              " 'steep': 916,\n",
              " 'resembling': 917,\n",
              " 'attending': 918,\n",
              " 'pilgrimage': 919,\n",
              " 'pitch': 920,\n",
              " 'converted': 921,\n",
              " 'going': 922,\n",
              " \"unlook'd\": 923,\n",
              " 'delights': 924,\n",
              " 'concord': 925,\n",
              " 'sounds': 926,\n",
              " 'married': 927,\n",
              " 'sweetly': 928,\n",
              " 'mutual': 929,\n",
              " 'pleasing': 930,\n",
              " 'note': 931,\n",
              " 'speechless': 932,\n",
              " 'sings': 933,\n",
              " 'wail': 934,\n",
              " 'widow': 935,\n",
              " 'bosom': 936,\n",
              " 'himself': 937,\n",
              " 'commits': 938,\n",
              " 'deny': 939,\n",
              " \"possess'd\": 940,\n",
              " 'chief': 941,\n",
              " 'presence': 942,\n",
              " 'growest': 943,\n",
              " 'folly': 944,\n",
              " 'bounty': 945,\n",
              " 'copy': 946,\n",
              " 'clock': 947,\n",
              " 'tells': 948,\n",
              " 'violet': 949,\n",
              " 'lofty': 950,\n",
              " 'canopy': 951,\n",
              " 'borne': 952,\n",
              " 'question': 953,\n",
              " 'wastes': 954,\n",
              " 'forsake': 955,\n",
              " 'defence': 956,\n",
              " 'coming': 957,\n",
              " 'prepare': 958,\n",
              " 'issue': 959,\n",
              " 'wind': 960,\n",
              " 'princes': 961,\n",
              " 'thrive': 962,\n",
              " 'perfection': 963,\n",
              " 'little': 964,\n",
              " 'huge': 965,\n",
              " 'stage': 966,\n",
              " 'influence': 967,\n",
              " 'comment': 968,\n",
              " 'youthful': 969,\n",
              " 'inconstant': 970,\n",
              " 'sets': 971,\n",
              " 'wasteful': 972,\n",
              " 'fortify': 973,\n",
              " 'counterfeit': 974,\n",
              " 'pencil': 975,\n",
              " 'neither': 976,\n",
              " 'deserts': 977,\n",
              " 'poet': 978,\n",
              " 'faces': 979,\n",
              " \"poet's\": 980,\n",
              " 'alive': 981,\n",
              " 'twice': 982,\n",
              " 'winds': 983,\n",
              " 'shines': 984,\n",
              " 'often': 985,\n",
              " 'gold': 986,\n",
              " 'changing': 987,\n",
              " 'possession': 988,\n",
              " 'breathe': 989,\n",
              " 'keen': 990,\n",
              " 'fierce': 991,\n",
              " 'glad': 992,\n",
              " 'seasons': 993,\n",
              " \"whate'er\": 994,\n",
              " 'fading': 995,\n",
              " 'allow': 996,\n",
              " 'pattern': 997,\n",
              " 'master': 998,\n",
              " 'acquainted': 999,\n",
              " 'fashion': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 1 to include the padding token (index 0), which is excluded from word_index\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "IAaZ5MeWhIlG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfSfwEn6IvQT",
        "outputId": "511b32c0-bcf9-4a09-939b-808b50c3d008"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3211"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2. Generate N-gram Sequences**"
      ],
      "metadata": {
        "id": "Z74028IZGKcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split each sentence into n-gram sequences of increasing length to create training samples"
      ],
      "metadata": {
        "id": "YJ0m4DVfHrWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "    # Convert the sentence into a sequence of token indices and extract the inner list\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        # Generate an n-gram sequence from the start up to position i (minimum 2 tokens)\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "n_kR1y1wGObV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoUPQyLIWC1J",
        "outputId": "cf9ca90d-0579-41b7-d6ac-fcb92c1c8a10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517],\n",
              " [8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[34, 417]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiVU0FyabDom",
        "outputId": "4b06c5b5-9328-40ca-8bb2-815a5406de1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from fairest']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[34, 417, 877]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hg3JFJfFje7",
        "outputId": "691d2d5e-a117-4e5e-b921-c33bfb8bf01e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from fairest creatures']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display training samples as text"
      ],
      "metadata": {
        "id": "t5acxil1cm1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for point in input_sequences[:10]:\n",
        "  # Convert the n-gram sequence of token indices back into text and join into a string\n",
        "  print(\" \".join(tokenizer.sequences_to_texts([point])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9057QwFabWbD",
        "outputId": "29f60d5f-704b-46d0-aced-9cf88f26f800"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fairest\n",
            "from fairest creatures\n",
            "from fairest creatures we\n",
            "from fairest creatures we desire\n",
            "from fairest creatures we desire increase\n",
            "that thereby\n",
            "that thereby beauty's\n",
            "that thereby beauty's rose\n",
            "that thereby beauty's rose might\n",
            "that thereby beauty's rose might never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3. Split Features and Labels**"
      ],
      "metadata": {
        "id": "5oFTxkCO40Qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate text from left to right, zero-padding is applied to the **beginning** of each training sample to ensure equal length"
      ],
      "metadata": {
        "id": "uaF3hjvm60S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "CVzre0i746Kk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qfFQ2IcGPKw",
        "outputId": "1040d272-d779-4e8b-9492-56b71be377f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[:10]"
      ],
      "metadata": {
        "id": "M7-oB8xeEe8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152ee795-eb68-4745-e286-c8e40dc340d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  34, 417, 877, 166],\n",
              "       [  0,   0,   0,   0,   0,   0,  34, 417, 877, 166, 213],\n",
              "       [  0,   0,   0,   0,   0,  34, 417, 877, 166, 213, 517],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134, 351, 102, 156]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the training samples into features and labels"
      ],
      "metadata": {
        "id": "zaiJhp2-E3L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "-HwrLI9qE6HG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictors[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb9gyuaBQKny",
        "outputId": "04b5d963-90bd-4e12-fd92-49272969a6ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  34],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,   0,   0,   0,   0,  34, 417, 877, 166],\n",
              "       [  0,   0,   0,   0,   0,  34, 417, 877, 166, 213],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   8],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134, 351, 102]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dk6EzVfPwBq",
        "outputId": "9d301f2f-7885-4614-cf7b-adb8d63e2be0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([417, 877, 166, 213, 517, 878, 134, 351, 102, 156], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(\"{} ---> {}\".format(\" \".join(tokenizer.sequences_to_texts([predictors[i]])), \" \".join(tokenizer.sequences_to_texts([[label[i]]]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ-XmkPfPxUN",
        "outputId": "4e47a781-99d8-4ee1-9534-677993526195"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from ---> fairest\n",
            "from fairest ---> creatures\n",
            "from fairest creatures ---> we\n",
            "from fairest creatures we ---> desire\n",
            "from fairest creatures we desire ---> increase\n",
            "that ---> thereby\n",
            "that thereby ---> beauty's\n",
            "that thereby beauty's ---> rose\n",
            "that thereby beauty's rose ---> might\n",
            "that thereby beauty's rose might ---> never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert each label into a one-hot vector based on the vocabulary size"
      ],
      "metadata": {
        "id": "SCfhstW5lrTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = ku.to_categorical(label, num_classes=total_words).astype(\"float32\")"
      ],
      "metadata": {
        "id": "o0GxH1Fzl2CQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu0MuR64mKZL",
        "outputId": "84bd32f5-87fb-40ab-c37b-947cad6b41fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(label[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg2tQecWGno6",
        "outputId": "8200743d-5c2c-437d-dc58-bafe0c3876e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Model Training**"
      ],
      "metadata": {
        "id": "RggOa9XWnlKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))\n",
        "model.add(SimpleRNN(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XSJvHk6nnqEb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(predictors, label, epochs=130, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6j5Oe_TdNjo",
        "outputId": "517d5ed6-1bee-4a24-d03c-8f04cb8398b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.0206 - loss: 7.0694\n",
            "Epoch 2/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.0358 - loss: 6.3432\n",
            "Epoch 3/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.0575 - loss: 5.9540\n",
            "Epoch 4/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.0714 - loss: 5.5557\n",
            "Epoch 5/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.0900 - loss: 5.1119\n",
            "Epoch 6/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.1267 - loss: 4.7142\n",
            "Epoch 7/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.1748 - loss: 4.3150\n",
            "Epoch 8/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.2428 - loss: 3.9486\n",
            "Epoch 9/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.3012 - loss: 3.6064\n",
            "Epoch 10/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.3674 - loss: 3.2727\n",
            "Epoch 11/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.4180 - loss: 2.9784\n",
            "Epoch 12/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.4665 - loss: 2.6960\n",
            "Epoch 13/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.5190 - loss: 2.4290\n",
            "Epoch 14/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.5597 - loss: 2.2280\n",
            "Epoch 15/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.6002 - loss: 2.0092\n",
            "Epoch 16/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6352 - loss: 1.8516\n",
            "Epoch 17/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.6648 - loss: 1.6968\n",
            "Epoch 18/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.6941 - loss: 1.5612\n",
            "Epoch 19/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7131 - loss: 1.4539\n",
            "Epoch 20/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.7394 - loss: 1.3172\n",
            "Epoch 21/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.7553 - loss: 1.2313\n",
            "Epoch 22/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.7688 - loss: 1.1616\n",
            "Epoch 23/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.7813 - loss: 1.0974\n",
            "Epoch 24/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.7940 - loss: 1.0098\n",
            "Epoch 25/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8055 - loss: 0.9649\n",
            "Epoch 26/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8060 - loss: 0.9348\n",
            "Epoch 27/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8196 - loss: 0.8705\n",
            "Epoch 28/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8225 - loss: 0.8465\n",
            "Epoch 29/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8282 - loss: 0.7920\n",
            "Epoch 30/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8284 - loss: 0.7982\n",
            "Epoch 31/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8358 - loss: 0.7642\n",
            "Epoch 32/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8413 - loss: 0.7220\n",
            "Epoch 33/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8459 - loss: 0.6975\n",
            "Epoch 34/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8463 - loss: 0.6789\n",
            "Epoch 35/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8482 - loss: 0.6610\n",
            "Epoch 36/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8490 - loss: 0.6550\n",
            "Epoch 37/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8498 - loss: 0.6386\n",
            "Epoch 38/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8519 - loss: 0.6281\n",
            "Epoch 39/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8534 - loss: 0.6190\n",
            "Epoch 40/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8480 - loss: 0.6308\n",
            "Epoch 41/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8527 - loss: 0.5985\n",
            "Epoch 42/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8541 - loss: 0.5904\n",
            "Epoch 43/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8537 - loss: 0.5925\n",
            "Epoch 44/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8512 - loss: 0.6040\n",
            "Epoch 45/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8586 - loss: 0.5694\n",
            "Epoch 46/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8519 - loss: 0.5815\n",
            "Epoch 47/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.8519 - loss: 0.5828\n",
            "Epoch 48/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8583 - loss: 0.5512\n",
            "Epoch 49/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8502 - loss: 0.5817\n",
            "Epoch 50/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8556 - loss: 0.5708\n",
            "Epoch 51/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8549 - loss: 0.5667\n",
            "Epoch 52/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8539 - loss: 0.5636\n",
            "Epoch 53/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.8565 - loss: 0.5531\n",
            "Epoch 54/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8542 - loss: 0.5542\n",
            "Epoch 55/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8578 - loss: 0.5453\n",
            "Epoch 56/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8548 - loss: 0.5455\n",
            "Epoch 57/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8538 - loss: 0.5504\n",
            "Epoch 58/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8514 - loss: 0.5545\n",
            "Epoch 59/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8542 - loss: 0.5489\n",
            "Epoch 60/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8542 - loss: 0.5418\n",
            "Epoch 61/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8530 - loss: 0.5595\n",
            "Epoch 62/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8518 - loss: 0.5446\n",
            "Epoch 63/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8552 - loss: 0.5471\n",
            "Epoch 64/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8533 - loss: 0.5417\n",
            "Epoch 65/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8497 - loss: 0.5508\n",
            "Epoch 66/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8591 - loss: 0.5314\n",
            "Epoch 67/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8547 - loss: 0.5411\n",
            "Epoch 68/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8555 - loss: 0.5354\n",
            "Epoch 69/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8534 - loss: 0.5515\n",
            "Epoch 70/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8558 - loss: 0.5350\n",
            "Epoch 71/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8545 - loss: 0.5341\n",
            "Epoch 72/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8597 - loss: 0.5179\n",
            "Epoch 73/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8575 - loss: 0.5186\n",
            "Epoch 74/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8584 - loss: 0.5200\n",
            "Epoch 75/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8540 - loss: 0.5268\n",
            "Epoch 76/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8577 - loss: 0.5087\n",
            "Epoch 77/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8579 - loss: 0.5272\n",
            "Epoch 78/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8516 - loss: 0.5455\n",
            "Epoch 79/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8531 - loss: 0.5305\n",
            "Epoch 80/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8502 - loss: 0.5436\n",
            "Epoch 81/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.8587 - loss: 0.5078\n",
            "Epoch 82/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8522 - loss: 0.5350\n",
            "Epoch 83/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8554 - loss: 0.5242\n",
            "Epoch 84/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8529 - loss: 0.5430\n",
            "Epoch 85/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8565 - loss: 0.5168\n",
            "Epoch 86/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8545 - loss: 0.5282\n",
            "Epoch 87/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8534 - loss: 0.5197\n",
            "Epoch 88/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.8561 - loss: 0.5141\n",
            "Epoch 89/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8578 - loss: 0.5075\n",
            "Epoch 90/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8534 - loss: 0.5381\n",
            "Epoch 91/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8538 - loss: 0.5203\n",
            "Epoch 92/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8552 - loss: 0.5190\n",
            "Epoch 93/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8526 - loss: 0.5365\n",
            "Epoch 94/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8567 - loss: 0.5188\n",
            "Epoch 95/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8548 - loss: 0.5243\n",
            "Epoch 96/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8539 - loss: 0.5229\n",
            "Epoch 97/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8611 - loss: 0.5035\n",
            "Epoch 98/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8524 - loss: 0.5254\n",
            "Epoch 99/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8505 - loss: 0.5382\n",
            "Epoch 100/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8559 - loss: 0.5116\n",
            "Epoch 101/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8527 - loss: 0.5274\n",
            "Epoch 102/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8505 - loss: 0.5295\n",
            "Epoch 103/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8575 - loss: 0.5050\n",
            "Epoch 104/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8577 - loss: 0.5019\n",
            "Epoch 105/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8575 - loss: 0.5124\n",
            "Epoch 106/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8550 - loss: 0.5175\n",
            "Epoch 107/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8591 - loss: 0.4982\n",
            "Epoch 108/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8579 - loss: 0.5067\n",
            "Epoch 109/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8614 - loss: 0.4925\n",
            "Epoch 110/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8527 - loss: 0.5184\n",
            "Epoch 111/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8588 - loss: 0.4961\n",
            "Epoch 112/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8591 - loss: 0.5031\n",
            "Epoch 113/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.8521 - loss: 0.5158\n",
            "Epoch 114/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8561 - loss: 0.5116\n",
            "Epoch 115/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.8578 - loss: 0.5130\n",
            "Epoch 116/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8594 - loss: 0.5002\n",
            "Epoch 117/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8593 - loss: 0.4843\n",
            "Epoch 118/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8575 - loss: 0.5097\n",
            "Epoch 119/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8515 - loss: 0.5225\n",
            "Epoch 120/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8523 - loss: 0.5129\n",
            "Epoch 121/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.8616 - loss: 0.4899\n",
            "Epoch 122/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8538 - loss: 0.5153\n",
            "Epoch 123/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8536 - loss: 0.5072\n",
            "Epoch 124/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.8518 - loss: 0.5207\n",
            "Epoch 125/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.8492 - loss: 0.5288\n",
            "Epoch 126/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8513 - loss: 0.5421\n",
            "Epoch 127/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8568 - loss: 0.5004\n",
            "Epoch 128/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8570 - loss: 0.4975\n",
            "Epoch 129/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8578 - loss: 0.5036\n",
            "Epoch 130/130\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8556 - loss: 0.4971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "qnjCayyrxjGE",
        "outputId": "890a7a92-a2e2-4b28-c2ef-0846afb41b30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m321,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m29,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3211\u001b[0m)           │       \u001b[38;5;34m414,219\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">321,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3211</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">414,219</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,293,895\u001b[0m (8.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,293,895</span> (8.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m764,631\u001b[0m (2.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">764,631</span> (2.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,529,264\u001b[0m (5.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,529,264</span> (5.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Text Generation**"
      ],
      "metadata": {
        "id": "GLDqvA-obDlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed text"
      ],
      "metadata": {
        "id": "oIylYBVdbd41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = 'despite of wrinkles'"
      ],
      "metadata": {
        "id": "Qmso0dsBa822"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from the seed text, iteratively predict and append the next word until the specified length is reached"
      ],
      "metadata": {
        "id": "J3s1zD8JeTgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "  # Convert the sentence into a sequence of token indices and extract the inner list\n",
        "  token_list = tokenizer.texts_to_sequences([test_seq])[0]\n",
        "\n",
        "  # Pad the text to match the model’s input length (minus 1 to exclude the label word)\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "\n",
        "  # Predict the next word's probability distribution\n",
        "  predicted = model.predict(token_list, verbose=0)\n",
        "\n",
        "  predicted_id = np.argmax(predicted[0])\n",
        "\n",
        "  output_word = \"\"\n",
        "\n",
        "  if predicted_id in tokenizer.index_word:\n",
        "    output_word = tokenizer.index_word[predicted_id]\n",
        "    if output_word == '<end>':\n",
        "      break\n",
        "    test_seq += \" \" + output_word\n",
        "  else:\n",
        "    break\n",
        "\n",
        "  print(test_seq)"
      ],
      "metadata": {
        "id": "HfIaQcK1eUUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0be33b-ed46-48db-c3b4-aaab1b29df9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "despite of wrinkles this\n",
            "despite of wrinkles this thy\n",
            "despite of wrinkles this thy golden\n",
            "despite of wrinkles this thy golden time\n",
            "despite of wrinkles this thy golden time my\n",
            "despite of wrinkles this thy golden time my deeds\n",
            "despite of wrinkles this thy golden time my deeds to\n",
            "despite of wrinkles this thy golden time my deeds to shame\n",
            "despite of wrinkles this thy golden time my deeds to shame deny\n",
            "despite of wrinkles this thy golden time my deeds to shame deny silent\n"
          ]
        }
      ]
    }
  ]
}